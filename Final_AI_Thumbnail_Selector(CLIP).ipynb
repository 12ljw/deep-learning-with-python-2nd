{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/12ljw/deep-learning-with-python-2nd/blob/main/Final_AI_Thumbnail_Selector(CLIP).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Training‑Free AI Thumbnail Selector (CLIP)\n",
        "\n",
        "유튜브 **영상 제목(텍스트)** 과 **추출 프레임(이미지)** 간 의미 유사도를 계산해, *학습 없이* 베스트 썸네일을 추천.\n",
        "\n",
        "**Pipeline**\n",
        "1) ffmpeg로 10초 간격 프레임 추출  \n",
        "2) CLIP 임베딩으로 제목–이미지 유사도 계산  \n",
        "3) Top‑k 썸네일 추천 (+ 선택: 유사 프레임 중복 제거)\n"
      ],
      "id": "title"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytube\n",
        "!pip install ImageHash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHmrGTHugAZS",
        "outputId": "e847aad5-464b-4691-c1f3-378e54a884ea"
      },
      "id": "tHmrGTHugAZS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n",
            "Collecting ImageHash\n",
            "  Downloading ImageHash-4.3.2-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.12/dist-packages (from ImageHash) (1.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from ImageHash) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from ImageHash) (11.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from ImageHash) (1.16.3)\n",
            "Downloading ImageHash-4.3.2-py2.py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ImageHash\n",
            "Successfully installed ImageHash-4.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YouTube 다운로드 설정\n",
        "!pip install -q yt-dlp\n",
        "import yt_dlp\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "YOUTUBE_URL = 'https://www.youtube.com/watch?v=dFO5O_jxUTk'\n",
        "OUTPUT_MP4 = 'input.mp4'\n",
        "\n",
        "ydl_opts = {\n",
        "    'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best',\n",
        "    'merge_output_format': 'mp4',\n",
        "    'outtmpl': 'yt_download.%(ext)s',\n",
        "    'noplaylist': True,\n",
        "}\n",
        "\n",
        "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "    info = ydl.extract_info(YOUTUBE_URL, download=True)\n",
        "    downloaded = ydl.prepare_filename(info)\n",
        "\n",
        "\n",
        "if os.path.exists(OUTPUT_MP4):\n",
        "    os.remove(OUTPUT_MP4)\n",
        "shutil.move(downloaded if os.path.exists(downloaded) else 'yt_download.mp4', OUTPUT_MP4)\n",
        "print('다운로드 완료:', OUTPUT_MP4)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWww_YoYCD5E",
        "outputId": "fa46ecb1-98a9-44e2-ac05-963e4826cd98"
      },
      "id": "lWww_YoYCD5E",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.0/180.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h[youtube] Extracting URL: https://www.youtube.com/watch?v=dFO5O_jxUTk\n",
            "[youtube] dFO5O_jxUTk: Downloading webpage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: [youtube] No supported JavaScript runtime could be found. YouTube extraction without a JS runtime has been deprecated, and some formats may be missing. See  https://github.com/yt-dlp/yt-dlp/wiki/EJS  for details on installing one. To silence this warning, you can use  --extractor-args \"youtube:player_client=default\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] dFO5O_jxUTk: Downloading android sdkless player API JSON\n",
            "[youtube] dFO5O_jxUTk: Downloading web safari player API JSON\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: [youtube] dFO5O_jxUTk: Some web_safari client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] dFO5O_jxUTk: Downloading m3u8 information\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: [youtube] dFO5O_jxUTk: Some web client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[info] dFO5O_jxUTk: Downloading 1 format(s): 399+140-1\n",
            "[download] Sleeping 5.00 seconds as required by the site...\n",
            "[download] Destination: yt_download.f399.mp4\n",
            "[download] 100% of  128.88MiB in 00:02:11 at 1005.43KiB/s\n",
            "[download] Destination: yt_download.f140-1.m4a\n",
            "[download] 100% of   14.47MiB in 00:00:09 at 1.59MiB/s   \n",
            "[Merger] Merging formats into \"yt_download.mp4\"\n",
            "Deleting original file yt_download.f140-1.m4a (pass -k to keep)\n",
            "Deleting original file yt_download.f399.mp4 (pass -k to keep)\n",
            "다운로드 완료: input.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ffmpeg로 프레임 추출 (10초 간격)\n",
        "import os\n",
        "\n",
        "os.makedirs(\"frames\", exist_ok=True)\n",
        "\n",
        "!ffmpeg -i input.mp4 -vf fps=1/10 frames/frame_%05d.jpg\n",
        "\n",
        "print(\"프레임 추출 완료.\")"
      ],
      "metadata": {
        "id": "_l-7S9SOCIY3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24c4461a-518e-49e9-a0ee-878eb24acf35"
      },
      "id": "_l-7S9SOCIY3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "\u001b[1;36m[libdav1d @ 0x55be9499f500] \u001b[0mlibdav1d 0.9.2\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'input.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomav01iso2mp41\n",
            "    encoder         : Lavf58.76.100\n",
            "  Duration: 00:15:37.74, start: 0.000000, bitrate: 1283 kb/s\n",
            "  Stream #0:0(und): Video: av1 (Main) (av01 / 0x31307661), yuv420p(tv, bt709), 1920x1080 [SAR 1:1 DAR 16:9], 1150 kb/s, 23.98 fps, 23.98 tbr, 24k tbn, 24k tbc (default)\n",
            "    Metadata:\n",
            "      handler_name    : ISO Media file produced by Google Inc.\n",
            "      vendor_id       : [0][0][0][0]\n",
            "  Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 128 kb/s (default)\n",
            "    Metadata:\n",
            "      handler_name    : ISO Media file produced by Google Inc.\n",
            "      vendor_id       : [0][0][0][0]\n",
            "\u001b[1;36m[libdav1d @ 0x55be949c2400] \u001b[0mlibdav1d 0.9.2\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (av1 (libdav1d) -> mjpeg (native))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;34m[swscaler @ 0x55be94bab580] \u001b[0m\u001b[0;33mdeprecated pixel format used, make sure you did set range correctly\n",
            "\u001b[0mOutput #0, image2, to 'frames/frame_%05d.jpg':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomav01iso2mp41\n",
            "    encoder         : Lavf58.76.100\n",
            "  Stream #0:0(und): Video: mjpeg, yuvj420p(pc, bt709, progressive), 1920x1080 [SAR 1:1 DAR 16:9], q=2-31, 200 kb/s, 0.10 fps, 0.10 tbn (default)\n",
            "    Metadata:\n",
            "      handler_name    : ISO Media file produced by Google Inc.\n",
            "      vendor_id       : [0][0][0][0]\n",
            "      encoder         : Lavc58.134.100 mjpeg\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/200000 buffer size: 0 vbv_delay: N/A\n",
            "frame=   94 fps=0.3 q=1.6 Lsize=N/A time=00:15:40.00 bitrate=N/A speed=2.91x    \n",
            "video:14271kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown\n",
            "프레임 추출 완료.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pHash 중복 프레임 제거\n",
        "from PIL import Image\n",
        "import imagehash\n",
        "import glob\n",
        "\n",
        "def deduplicate_frames(frame_dir, threshold=5):\n",
        "    frame_paths = sorted(glob.glob(os.path.join(frame_dir, \"*.jpg\")))\n",
        "    kept = []\n",
        "    hashes = []\n",
        "\n",
        "    for path in frame_paths:\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        h = imagehash.phash(img)\n",
        "\n",
        "        if not any(abs(h - old_h) <= threshold for old_h in hashes):\n",
        "            kept.append(path)\n",
        "            hashes.append(h)\n",
        "\n",
        "    print(f\"Total frames: {len(frame_paths)}, Kept: {len(kept)}\")\n",
        "    return kept\n",
        "\n",
        "frame_dir = \"frames\"\n",
        "kept_frames = deduplicate_frames(frame_dir, threshold=5)\n",
        "\n",
        "len(kept_frames)"
      ],
      "metadata": {
        "id": "T43x4eTbCpe3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef9b8dc6-58fb-45e3-ee65-0c9251dd8de9"
      },
      "id": "T43x4eTbCpe3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total frames: 94, Kept: 92\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "92"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ftfy regex tqdm git+https://github.com/openai/CLIP.git\n",
        "\n",
        "import torch\n",
        "import clip\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device)\n",
        "\n",
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)"
      ],
      "metadata": {
        "id": "T79Y2JaXB-F1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc5d0ebf-d973-4952-e412-aa2e0b319aa6"
      },
      "id": "T79Y2JaXB-F1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-7qorpr6v\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-7qorpr6v\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy) (0.2.14)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (25.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (0.24.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->clip==1.0) (3.0.3)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=d95cc2ecf3fb09cb990b19d15aa93fd4684bec0ef30636978ec609d85b2d8452\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-scs7egie/wheels/35/3e/df/3d24cbfb3b6a06f17a2bfd7d1138900d4365d9028aa8f6e92f\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.3.1\n",
            "device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:03<00:00, 97.9MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 얼굴 검출 algorithm (Haar cascade)\n",
        "face_cascade = cv2.CascadeClassifier(\n",
        "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
        ")\n",
        "\n",
        "def brightness_score(img_bgr):\n",
        "    # 이미지 전체 밝기 (0~1)\n",
        "    hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)\n",
        "    return hsv[..., 2].mean() / 255.0\n",
        "\n",
        "def complexity_score(img_bgr):\n",
        "    # 엣지 양으로 복잡도 측정 (0~1, 높을수록 복잡)\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    edges = cv2.Canny(gray, 100, 200)\n",
        "    return edges.mean() / 255.0\n",
        "\n",
        "def center_focus_score(img_bgr):\n",
        "    # 화면 중앙부에 정보(edge)가 얼마나 모여 있는지 (0~1)\n",
        "    h, w, _ = img_bgr.shape\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    edges = cv2.Canny(gray, 100, 200) / 255.0\n",
        "\n",
        "    ch, cw = int(h * 0.5), int(w * 0.5)\n",
        "    y1, y2 = (h - ch) // 2, (h + ch) // 2\n",
        "    x1, x2 = (w - cw) // 2, (w + cw) // 2\n",
        "\n",
        "    center_region = edges[y1:y2, x1:x2]\n",
        "    return center_region.mean()\n",
        "\n",
        "def face_score(img_bgr):\n",
        "    # 얼굴이 잘 보일수록 score 높음\n",
        "    # 얼굴 영역 / 전체 영역 비율을 0~1로 스케일링\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(\n",
        "        gray,\n",
        "        scaleFactor=1.1,\n",
        "        minNeighbors=5,\n",
        "        minSize=(40, 40)\n",
        "    )\n",
        "\n",
        "    if len(faces) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    h, w, _ = img_bgr.shape\n",
        "    img_area = h * w\n",
        "\n",
        "    face_area_sum = 0\n",
        "    for (x, y, fw, fh) in faces:\n",
        "        face_area_sum += fw * fh\n",
        "\n",
        "    ratio = face_area_sum / img_area   # 얼굴 영역 비율\n",
        "    return float(min(ratio * 3.0, 1.0))   # 0~1 사이로 조정"
      ],
      "metadata": {
        "id": "V1bCvEZj8hYm"
      },
      "id": "V1bCvEZj8hYm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score_frames_clip_only(frame_paths, title_text):\n",
        "    # Baseline : CLIP 유사도만 사용하는 Frame Score\n",
        "    with torch.no_grad():\n",
        "        text_tokens = clip.tokenize([title_text]).to(device)\n",
        "        text_feat = clip_model.encode_text(text_tokens)\n",
        "        text_feat /= text_feat.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for path in tqdm(frame_paths, desc=\"CLIP-only scoring\"):\n",
        "        pil = Image.open(path).convert(\"RGB\")\n",
        "        img_tensor = clip_preprocess(pil).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            img_feat = clip_model.encode_image(img_tensor)\n",
        "            img_feat /= img_feat.norm(dim=-1, keepdim=True)\n",
        "            sim = (img_feat @ text_feat.T).item()\n",
        "\n",
        "        results.append({\n",
        "            \"path\": path,\n",
        "            \"clip\": sim\n",
        "        })\n",
        "\n",
        "    return sorted(results, key=lambda x: x[\"clip\"], reverse=True)\n",
        "\n",
        "\n",
        "def score_frames_clip_quality(frame_paths, title_text):\n",
        "    # CLIP + 밝기 + 중앙부 집중도 + 복잡도 + 얼굴 score 를 합친 Thumbnail Score\n",
        "    with torch.no_grad():\n",
        "        text_tokens = clip.tokenize([title_text]).to(device)\n",
        "        text_feat = clip_model.encode_text(text_tokens)\n",
        "        text_feat /= text_feat.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for path in tqdm(frame_paths, desc=\"CLIP+Quality scoring\"):\n",
        "        pil = Image.open(path).convert(\"RGB\")\n",
        "        img_tensor = clip_preprocess(pil).unsqueeze(0).to(device)\n",
        "        img_bgr = cv2.imread(path)\n",
        "\n",
        "        # 1) CLIP 유사도\n",
        "        with torch.no_grad():\n",
        "            img_feat = clip_model.encode_image(img_tensor)\n",
        "            img_feat /= img_feat.norm(dim=-1, keepdim=True)\n",
        "            sim = (img_feat @ text_feat.T).item()\n",
        "\n",
        "        # 2) 시각적 quality score\n",
        "        b  = brightness_score(img_bgr)\n",
        "        c  = complexity_score(img_bgr)\n",
        "        f  = center_focus_score(img_bgr)\n",
        "        fs = face_score(img_bgr)\n",
        "\n",
        "        # 3) 가중치\n",
        "        alpha = 0.55  # CLIP similarity\n",
        "        beta  = 0.10  # brightness\n",
        "        gamma = 0.10  # center focus\n",
        "        delta = 0.10  # complexity\n",
        "        eps   = 0.15  # face score\n",
        "\n",
        "        thumbnail_score = (\n",
        "            alpha * sim +\n",
        "            beta  * b +\n",
        "            gamma * f -\n",
        "            delta * c +\n",
        "            eps   * fs\n",
        "        )\n",
        "\n",
        "        results.append({\n",
        "            \"path\": path,\n",
        "            \"clip\": sim,\n",
        "            \"brightness\": b,\n",
        "            \"complexity\": c,\n",
        "            \"focus\": f,\n",
        "            \"face\": fs,\n",
        "            \"score\": thumbnail_score\n",
        "        })\n",
        "\n",
        "    return sorted(results, key=lambda x: x[\"score\"], reverse=True)"
      ],
      "metadata": {
        "id": "vzopkhfV9KMO"
      },
      "id": "vzopkhfV9KMO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_face_focus_blur(img_bgr, ksize=51):\n",
        "    # 프레임 내에\n",
        "    # 1) 얼굴이 있으면: 얼굴을 감싸는 영역만 강조, 나머지 블러처리\n",
        "    # 2) 얼굴이 없으면: 화면 중앙 기준으로 블러처리\n",
        "    h, w, _ = img_bgr.shape\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(\n",
        "        gray,\n",
        "        scaleFactor=1.1,\n",
        "        minNeighbors=5,\n",
        "        minSize=(40, 40)\n",
        "    )\n",
        "\n",
        "    if len(faces) == 0:\n",
        "        # 프레임 내에 얼굴이 없으면 프레임 중앙을 기준으로 blurring\n",
        "        ch, cw = int(h * 0.5), int(w * 0.5)\n",
        "        y1, y2 = (h - ch)//2, (h + ch)//2\n",
        "        x1, x2 = (w - cw)//2, (w + cw)//2\n",
        "\n",
        "        blurred = cv2.GaussianBlur(img_bgr, (ksize, ksize), 0)\n",
        "        out = blurred.copy()\n",
        "        out[y1:y2, x1:x2] = img_bgr[y1:y2, x1:x2]\n",
        "        return out\n",
        "\n",
        "    # 프레임 내에 얼굴이 여러 개일 경우: 전체를 감싸는 큰 박스 생성\n",
        "    x_min = min([x for (x, y, fw, fh) in faces])\n",
        "    y_min = min([y for (x, y, fw, fh) in faces])\n",
        "    x_max = max([x + fw for (x, y, fw, fh) in faces])\n",
        "    y_max = max([y + fh for (x, y, fw, fh) in faces])\n",
        "\n",
        "    margin_x = int(0.1 * w)\n",
        "    margin_y = int(0.1 * h)\n",
        "    x1 = max(0, x_min - margin_x)\n",
        "    y1 = max(0, y_min - margin_y)\n",
        "    x2 = min(w, x_max + margin_x)\n",
        "    y2 = min(h, y_max + margin_y)\n",
        "\n",
        "    blurred = cv2.GaussianBlur(img_bgr, (ksize, ksize), 0)\n",
        "    out = blurred.copy()\n",
        "    out[y1:y2, x1:x2] = img_bgr[y1:y2, x1:x2]\n",
        "    return out"
      ],
      "metadata": {
        "id": "ulgv8deY9ozO"
      },
      "id": "ulgv8deY9ozO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "TITLE = \"Choo Sung Hoon Living in a Rented Room in SHIHO's House\"\n",
        "\n",
        "# 1) Baseline & Thumbnail Score 계산\n",
        "clip_only = score_frames_clip_only(kept_frames, TITLE)\n",
        "clip_qual = score_frames_clip_quality(kept_frames, TITLE)\n",
        "\n",
        "print(\"=== CLIP-only Top 3 ===\")\n",
        "for i, s in enumerate(clip_only[:3], start=1):\n",
        "    print(f\"[{i}] {os.path.basename(s['path'])}  |  CLIP={s['clip']:.3f}\")\n",
        "\n",
        "print(\"\\n=== CLIP+Quality(+Face) Top 3 ===\")\n",
        "for i, s in enumerate(clip_qual[:3], start=1):\n",
        "    print(\n",
        "        f\"[{i}] {os.path.basename(s['path'])}  |  Score={s['score']:.3f}  \"\n",
        "        f\"(CLIP={s['clip']:.3f}, B={s['brightness']:.3f}, \"\n",
        "        f\"F={s['focus']:.3f}, C={s['complexity']:.3f}, Face={s['face']:.3f})\"\n",
        "    )"
      ],
      "metadata": {
        "id": "2bmGJAwS9zpZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ca1fd37-c499-4e0a-86b8-bfe42bede51b"
      },
      "id": "2bmGJAwS9zpZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CLIP-only scoring: 100%|██████████| 92/92 [00:29<00:00,  3.16it/s]\n",
            "CLIP+Quality scoring: 100%|██████████| 92/92 [01:26<00:00,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CLIP-only Top 3 ===\n",
            "[1] frame_00047.jpg  |  CLIP=0.361\n",
            "[2] frame_00079.jpg  |  CLIP=0.355\n",
            "[3] frame_00068.jpg  |  CLIP=0.333\n",
            "\n",
            "=== CLIP+Quality(+Face) Top 3 ===\n",
            "[1] frame_00037.jpg  |  Score=0.278  (CLIP=0.318, B=0.445, F=0.001, C=0.006, Face=0.397)\n",
            "[2] frame_00079.jpg  |  Score=0.277  (CLIP=0.355, B=0.689, F=0.013, C=0.007, Face=0.083)\n",
            "[3] frame_00024.jpg  |  Score=0.270  (CLIP=0.322, B=0.634, F=0.029, C=0.033, Face=0.198)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "# 1) Baseline(CLIP-only) Top-1 Thumbnail\n",
        "best_clip = clip_only[0]\n",
        "img_clip = cv2.imread(best_clip[\"path\"])\n",
        "cv2.imwrite(\"results/best_clip_only.jpg\", img_clip)\n",
        "\n",
        "# 2) 개선 버전 Top-1 (원본 + 얼굴 중심 블러)\n",
        "best_qual = clip_qual[0]\n",
        "img_qual = cv2.imread(best_qual[\"path\"])\n",
        "img_qual_face = apply_face_focus_blur(img_qual, ksize=51)\n",
        "\n",
        "cv2.imwrite(\"results/best_quality_raw.jpg\", img_qual)\n",
        "cv2.imwrite(\"results/best_quality_face_focus.jpg\", img_qual_face)\n",
        "\n",
        "print(\"저장된 파일:\")\n",
        "print(\"- results/best_clip_only.jpg\")\n",
        "print(\"- results/best_quality_raw.jpg\")\n",
        "print(\"- results/best_quality_face_focus.jpg\")"
      ],
      "metadata": {
        "id": "76BfPL8MA443",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fe91353-191d-4ba4-e600-baac0a5137b2"
      },
      "id": "76BfPL8MA443",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "저장된 파일:\n",
            "- results/best_clip_only.jpg\n",
            "- results/best_quality_raw.jpg\n",
            "- results/best_quality_face_focus.jpg\n"
          ]
        }
      ]
    }
  ]
}